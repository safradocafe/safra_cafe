# pages/4_1. An√°lise_de_correla√ß√£o.py
import os, glob, io, csv
from datetime import datetime

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from scipy.stats import shapiro, pearsonr

# =========================
# P√°gina / estilo
# =========================
st.set_page_config(page_title="An√°lise de correla√ß√£o", layout="wide")
st.markdown("## üìä An√°lise de Correla√ß√£o entre √≠ndices espectrais e produtividade")

BASE_TMP = "/tmp/streamlit_dados"
TOKENS_IDX = ["NDVI", "GNDVI", "NDRE", "CCCI", "MSAVI2", "NDWI", "NDMI", "NBR", "TWI2"]

# =========================
# Utilit√°rios
# =========================
def _find_latest_indices_csv(base=BASE_TMP):
    """Encontra o CSV mais recente gerado pela aba de Processamento/Previs√£o."""
    if not os.path.isdir(base):
        return None
    pats = glob.glob(os.path.join(base, "salvamento-*", "indices_espectrais_pontos_*.csv"))
    if not pats:
        return None
    pats.sort(key=lambda p: os.path.getmtime(p), reverse=True)
    return pats[0]

def _sniff_delim_and_decimal(sample_bytes: bytes):
    """Infere delimitador e separador decimal (v√≠rgula/ponto)."""
    text = sample_bytes.decode("utf-8", errors="ignore")
    # delimitador
    try:
        dialect = csv.Sniffer().sniff(text[:10000], delimiters=[",", ";", "\t", "|"])
        delim = dialect.delimiter
    except Exception:
        delim = ";" if text.count(";") > text.count(",") else ","
    # decimal
    decimal = "."
    for line in text.splitlines()[:50]:
        if any(ch.isdigit() for ch in line):
            if "," in line and (delim == ";" or line.count(",") > line.count(".")):
                decimal = ","
                break
    return delim, decimal

@st.cache_data(show_spinner=False)
def _read_csv_robusto_cached(raw: bytes):
    """L√™ CSV a partir de bytes com detec√ß√£o de separador/decimal; cacheado por conte√∫do."""
    delim, dec = _sniff_delim_and_decimal(raw)
    df = None
    for enc in ("utf-8", "latin-1"):
        try:
            df = pd.read_csv(io.BytesIO(raw), sep=delim, decimal=dec, encoding=enc)
            break
        except Exception:
            pass
    if df is None:
        df = pd.read_csv(io.BytesIO(raw))

    # caso tenha ficado tudo em 1 coluna, tenta o outro separador
    if df.shape[1] == 1:
        other = ";" if delim == "," else ","
        for enc in ("utf-8", "latin-1"):
            try:
                df = pd.read_csv(io.BytesIO(raw), sep=other, decimal=dec, encoding=enc)
                break
            except Exception:
                pass

    df.columns = [str(c).strip() for c in df.columns]
    df = df.dropna(axis=1, how="all")
    return df

@st.cache_data(show_spinner=False)
def _prepare_for_corr_cached(raw: bytes):
    """Trata o CSV bruto e retorna df pronto para correla√ß√£o (cacheado)."""
    df = _read_csv_robusto_cached(raw)

    # remove colunas extras
    drop_cols = [c for c in ["Code", "latitude", "longitude", "geometry"] if c in df.columns]
    df = df.drop(columns=drop_cols, errors="ignore")

    # seleciona 'maduro_kg' + TODAS as colunas de √≠ndices
    idx_cols = [c for c in df.columns if any(tok in c for tok in TOKENS_IDX)]
    keep = (["maduro_kg"] if "maduro_kg" in df.columns else []) + idx_cols
    df = df[keep].copy()

    # coer√ß√£o num√©rica
    for c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    # remove linhas sem alvo
    if "maduro_kg" in df.columns:
        df = df.dropna(subset=["maduro_kg"], how="any")

    # remove colunas totalmente NaN
    df = df.dropna(axis=1, how="all")
    return df

def _choose_method(df: pd.DataFrame):
    """Testa normalidade (Shapiro) e decide Pearson (se >50% normais) ou Spearman."""
    cols = [c for c in df.columns if c != "maduro_kg"]
    test_cols = ["maduro_kg"] + cols
    normal, tested = 0, 0
    for c in test_cols:
        s = df[c].dropna()
        if len(s) < 3:
            continue
        tested += 1
        try:
            _, p = shapiro(s)
            if p > 0.05:
                normal += 1
        except Exception:
            pass
    prop = (normal / tested) if tested else 0.0
    metodo = "pearson" if prop > 0.5 else "spearman"
    return metodo, prop

@st.cache_data(show_spinner=False)
def _compute_corr_all_cached(df_bytes: bytes):
    """
    Computa m√©todo, propor√ß√£o de normalidade, matriz de correla√ß√£o,
    s√©rie completa de correla√ß√µes com maduro_kg, top5 e p-valores (se Pearson).
    Cacheia pelo conte√∫do serializado do DF (bytes).
    """
    # reconstr√≥i df tratado a partir dos mesmos bytes
    df = _prepare_for_corr_cached(df_bytes)
    if df.empty or "maduro_kg" not in df.columns:
        return None

    metodo, prop_norm = _choose_method(df)
    corr = df.corr(method=metodo)

    # correla√ß√µes com o alvo
    cols = [c for c in df.columns if c != "maduro_kg"]
    vals = {}
    for c in cols:
        try:
            vals[c] = df[["maduro_kg", c]].corr(method=metodo).iloc[0, 1]
        except Exception:
            vals[c] = np.nan
    s_all = pd.Series(vals).dropna()
    top5_cols = s_all.abs().sort_values(ascending=False).head(5).index.tolist()

    pvals = {}
    if metodo == "pearson":
        a = pd.to_numeric(df["maduro_kg"], errors="coerce").dropna()
        for c in top5_cols:
            b = pd.to_numeric(df[c], errors="coerce").dropna()
            m = min(len(a), len(b))
            if m < 3:
                pvals[c] = np.nan
            else:
                try:
                    _, p = pearsonr(a.iloc[:m], b.iloc[:m])
                    pvals[c] = p
                except Exception:
                    pvals[c] = np.nan

    # serializa√ß√£o leve para n√£o duplicar DataFrame inteiro no cache
    return {
        "method": metodo,
        "prop_norm": prop_norm,
        "corr": corr,
        "s_all": s_all,
        "top5": top5_cols,
        "pvals": pvals,
    }

# =========================
# 1) Carregamento do CSV (auto + upload) ‚Äî com cache por BYTES
# =========================
st.markdown("#### 1) Carregamento de dados")

latest_csv_path = _find_latest_indices_csv()
raw_bytes = None

# autom√°tico
if latest_csv_path and os.path.exists(latest_csv_path):
    with open(latest_csv_path, "rb") as f:
        raw_bytes = f.read()
    st.success(f"‚úÖ CSV carregado automaticamente")

# upload manual (se existir, sobrescreve o autom√°tico)
up = st.file_uploader("Ou selecione manualmente o CSV gerado na aba de Processamento", type=["csv"])
if up is not None:
    raw_bytes = up.getvalue()
    st.info("CSV enviado via upload foi carregado e ser√° usado nesta an√°lise.")

if not raw_bytes:
    st.error("‚ùå Nenhum CSV v√°lido foi encontrado/enviado. Gere o arquivo na aba **Previs√£o da safra**.")
    st.stop()

with st.expander("Pr√©-visualiza√ß√£o do CSV (bruto)"):
    df_raw_preview = _read_csv_robusto_cached(raw_bytes)
    st.dataframe(df_raw_preview.head(), use_container_width=True)

# =========================
# 2) Tratamento: mant√©m 'maduro_kg' + TODOS os √≠ndices (cache)
# =========================
df = _prepare_for_corr_cached(raw_bytes)
if df.empty or "maduro_kg" not in df.columns:
    st.error("‚ùå N√£o foi poss√≠vel encontrar 'maduro_kg' e/ou colunas de √≠ndices espectrais no CSV.")
    st.stop()

st.caption(f"Linhas ap√≥s tratamento: **{len(df)}** | Colunas de an√°lise: **{len(df.columns)}**")
with st.expander("Visualizar dados tratados (maduro_kg + TODOS os √≠ndices)"):
    st.dataframe(df.head(), use_container_width=True)

# =========================
# 3) An√°lise autom√°tica (cache)
# =========================
st.markdown("### 2) An√°lise estat√≠stica (autom√°tica)")
res = _compute_corr_all_cached(raw_bytes)
if res is None:
    st.error("N√£o foi poss√≠vel calcular as correla√ß√µes.")
    st.stop()

metodo = res["method"]
prop_norm = res["prop_norm"]
corr = res["corr"]
s_all = res["s_all"]
top5_cols = res["top5"]
pvals = res["pvals"]

st.write(f"**M√©todo selecionado:** {'Pearson' if metodo=='pearson' else 'Spearman'} "
         f"(vari√°veis com normalidade: {prop_norm:.0%})")

st.subheader("Matriz de correla√ß√£o")
st.dataframe(corr.style.background_gradient(cmap="RdYlGn").format("{:.2f}"),
             use_container_width=True)

st.subheader("Top 5 correla√ß√µes (|r|) com **maduro_kg**")
for c in top5_cols:
    r = s_all[c]
    col1, col2 = st.columns([1, 4])
    with col1:
        st.metric(label=c, value=f"{r:.3f}")
    with col2:
        if metodo == "pearson":
            p = pvals.get(c, np.nan)
            if np.isnan(p):
                st.caption("p-valor: n/d")
            else:
                sig = "‚úÖ Significativa" if p < 0.05 else "‚ö†Ô∏è N√£o significativa"
                st.caption(f"p-valor: {p:.4f} ({sig})")

with st.expander("üìö Como interpretar os resultados"):
    st.markdown("""
- **Pearson**: rela√ß√£o linear (pressup√µe normalidade).  
- **Spearman**: rela√ß√£o monot√¥nica (robusto a outliers, n√£o exige normalidade).  
- **p-valor** (quando Pearson): < 0.05 sugere signific√¢ncia estat√≠stica.  
- Correla√ß√£o **n√£o** implica causalidade ‚Äî use como etapa explorat√≥ria.
""")

